{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josh1593/clrs-solutions/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t24Ib675VDRt",
        "outputId": "6c09ccb9-b836-433a-a4ec-33fc1e6770cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('drive/MyDrive')"
      ],
      "metadata": {
        "id": "shFJvhahVLiK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('cse_571_grasp')"
      ],
      "metadata": {
        "id": "p7HwRjGWVvQr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('pc_data', 'rb') as f:\n",
        "    pc_data, grasps_data, labels_data = pickle.load(f)\n",
        "\n",
        "with open('./test_pc_data','rb') as g:\n",
        "    test_pc_data, test_grasp_data, test_labels_data = pickle.load(g)\n"
      ],
      "metadata": {
        "id": "4jw_GN6dVars"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def normalize_point_cloud(points, fixed_size=1024):\n",
        "    if len(points) < fixed_size:\n",
        "        # Pad with zeros if less than fixed_size\n",
        "        padding = np.zeros((fixed_size - len(points), 3))\n",
        "        points = np.vstack((points, padding))\n",
        "    elif len(points) > fixed_size:\n",
        "        # Randomly sample if more than fixed_size\n",
        "        indices = np.random.choice(len(points), fixed_size, replace=False)\n",
        "        points = points[indices]\n",
        "    return points\n",
        "\n",
        "def normalize_pc_data(pc_data, fixed_size=1024):\n",
        "    normalized_pc_data = [normalize_point_cloud(points, fixed_size) for points in pc_data]\n",
        "    return normalized_pc_data\n",
        "\n",
        "fixed_size = 1024\n",
        "normalized_pc_data = normalize_pc_data(pc_data, fixed_size)\n",
        "normalized_test_data = normalize_pc_data(test_pc_data, fixed_size)\n"
      ],
      "metadata": {
        "id": "Z93I4cJyV1Xv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class GraspDataset(Dataset):\n",
        "    def __init__(self, point_clouds, grasps, labels, region_size=0.1, max_points=1024):\n",
        "        self.point_clouds = point_clouds\n",
        "        self.grasps = grasps\n",
        "        self.labels = labels\n",
        "        self.region_size = region_size\n",
        "        self.max_points = max_points\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.point_clouds)\n",
        "\n",
        "    def transform_to_local(self, point_cloud, grasp):\n",
        "        # Assuming grasp is a 4x4 transformation matrix\n",
        "        transformation_matrix = np.linalg.inv(grasp)\n",
        "        homogenous_coords = np.hstack((point_cloud, np.ones((point_cloud.shape[0], 1))))\n",
        "        transformed_coords = homogenous_coords @ transformation_matrix.T\n",
        "        return transformed_coords[:, :3]\n",
        "\n",
        "    def normalize(self, point_cloud):\n",
        "        centroid = np.mean(point_cloud, axis=0)\n",
        "        point_cloud -= centroid\n",
        "        max_dist = np.max(np.sqrt(np.sum(point_cloud ** 2, axis=1)))\n",
        "        point_cloud /= max_dist\n",
        "        return point_cloud\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      point_cloud = self.point_clouds[idx]\n",
        "      grasp = self.grasps[idx]\n",
        "      label = self.labels[idx]\n",
        "\n",
        "      # Transform point cloud to gripper's local coordinate system\n",
        "      transformed_point_cloud = self.transform_to_local(point_cloud, grasp)\n",
        "\n",
        "      # Normalize the point cloud\n",
        "      normalized_point_cloud = self.normalize(transformed_point_cloud)\n",
        "\n",
        "      # Ensure the point cloud has 3 channels\n",
        "      normalized_point_cloud = normalized_point_cloud[:, :3]  # Keep only the XYZ coordinates\n",
        "\n",
        "      return torch.from_numpy(normalized_point_cloud), torch.tensor(label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_points = max(item[0].shape[0] for item in batch)\n",
        "    max_points = min(max_points, 1024)  # Ensure we don't exceed the max_points limit\n",
        "\n",
        "    point_clouds = []\n",
        "    labels = []\n",
        "\n",
        "    for point_cloud, label in batch:\n",
        "        if point_cloud.shape[0] > max_points:\n",
        "            point_cloud = point_cloud[:max_points, :]\n",
        "        else:\n",
        "            padding = max_points - point_cloud.shape[0]\n",
        "            point_cloud = torch.cat([point_cloud, torch.zeros(padding, 3)], dim=0)\n",
        "\n",
        "        # Ensure the input tensor has 3 channels\n",
        "        point_cloud = point_cloud.permute(1, 0).unsqueeze(0)  # Transpose and add batch dimension\n",
        "        point_clouds.append(point_cloud)\n",
        "        labels.append(label)\n",
        "\n",
        "    point_clouds = torch.cat(point_clouds, dim=0)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)  # Convert labels to tensor and add a singleton dimension\n",
        "\n",
        "    return point_clouds, labels\n"
      ],
      "metadata": {
        "id": "JIfjkPHFV7pD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = GraspDataset(normalized_pc_data, grasps_data, labels_data)\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "test_dataset = GraspDataset(normalized_test_data, test_grasp_data, test_labels_data)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 128, shuffle = True, collate_fn = collate_fn)\n"
      ],
      "metadata": {
        "id": "0v4rfIrsWBqN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class PointNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PointNet, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)  # Output a single value for binary classification\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)  # Increase dropout to 50%\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.mp1 = nn.AdaptiveMaxPool1d(1)  # Global max pooling layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(2, 1)  # Transpose input tensor to correct shape\n",
        "        x = self.relu(self.bn1(self.conv1(x.float())))  # Convert input tensor to float data type\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.mp1(x)  # Apply global max pooling\n",
        "        x = x.view(-1, 1024)  # Flatten the tensor\n",
        "        x = self.dropout(self.relu(self.bn4(self.fc1(x))))  # Apply dropout\n",
        "        x = self.dropout(self.relu(self.bn5(self.fc2(x))))  # Apply dropout\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def train_and_evaluate(net, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=100, patience=10):\n",
        "    best_loss = float('inf')\n",
        "    early_stop_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "            inputs = inputs.transpose(2, 1).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs).squeeze()\n",
        "\n",
        "            if torch.isnan(outputs).any():\n",
        "                print(\"NaN values found in outputs\")\n",
        "                continue\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            if torch.isnan(loss).any():\n",
        "                print(\"NaN values found in loss\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)  # Clip gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        # Calculate the average training loss for this epoch\n",
        "        train_losses.append(running_loss / num_batches)\n",
        "\n",
        "        # Validation phase\n",
        "        net.eval()\n",
        "        val_running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.transpose(2, 1).to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = net(inputs).squeeze()\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_running_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        # Step the scheduler with the validation loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "    return train_losses, val_losses\n",
        "\n",
        "learning_rate = 0.0001\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = PointNet().to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-4)  # Add weight decay\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "\n",
        "train_losses, val_losses = train_and_evaluate(net, dataloader, test_dataloader, criterion, optimizer, scheduler, num_epochs=100)\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss over Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2lzzqgM6WPO-",
        "outputId": "fcd0ab7f-ec9d-4d75-9ef8-ce126146b336",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
            "<ipython-input-10-9c4dce6a0018>:45: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  return torch.from_numpy(normalized_point_cloud), torch.tensor(label)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 0.6180, Val Loss: 0.5889, Val Accuracy: 61.95%\n",
            "Epoch [2/100], Train Loss: 0.5907, Val Loss: 0.5729, Val Accuracy: 64.20%\n",
            "Epoch [3/100], Train Loss: 0.5776, Val Loss: 0.5718, Val Accuracy: 63.75%\n",
            "Epoch [4/100], Train Loss: 0.5679, Val Loss: 0.5748, Val Accuracy: 64.54%\n",
            "Epoch [5/100], Train Loss: 0.5614, Val Loss: 0.5544, Val Accuracy: 65.75%\n",
            "Epoch [6/100], Train Loss: 0.5544, Val Loss: 0.5580, Val Accuracy: 64.21%\n",
            "Epoch [7/100], Train Loss: 0.5496, Val Loss: 0.5509, Val Accuracy: 63.12%\n",
            "Epoch [8/100], Train Loss: 0.5465, Val Loss: 0.5372, Val Accuracy: 65.97%\n",
            "Epoch [9/100], Train Loss: 0.5436, Val Loss: 0.5443, Val Accuracy: 67.83%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}